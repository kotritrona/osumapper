{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### osu!nn #3: momentum estimator\n",
    "\n",
    "Builds up a model to estimate the momentum - distance over time interval, from the training data.\n",
    "\n",
    "It might be more accurate to be proportional to circle size, might implement later.\n",
    "\n",
    "Synthesis of \"momentumModel\"\n",
    "* rhythmData x 1\n",
    "* (Audio) x 1\n",
    "* (Regressor) x 1\n",
    "\n",
    "Synthesis Time: ~5 mins\n",
    "\n",
    "Final edit: 2018/8/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "root = \"mapdata/\";\n",
    "\n",
    "divisor = 4;\n",
    "\n",
    "# this is a global variable!\n",
    "time_interval = 16;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# lst file, [TICK, TIME, NOTE, IS_CIRCLE, IS_SLIDER, IS_SPINNER, IS_SLIDER_END, IS_SPINNER_END, \n",
    "#               0,    1,    2,         3,         4,          5,             6,              7,\n",
    "#            SLIDING, SPINNING, MOMENTUM, ANGULAR_MOMENTUM, EX1, EX2, EX3], length MAPTICKS\n",
    "#                  8,        9,       10,               11,  12,  13,  14,\n",
    "# wav file, [len(snapsize), MAPTICKS, 2, fft_size//4]\n",
    "def read_npz(fn):\n",
    "    with np.load(fn) as data:\n",
    "        wav_data = data[\"wav\"];\n",
    "        wav_data = np.swapaxes(wav_data, 2, 3);\n",
    "        train_data = wav_data;\n",
    "        div_source = data[\"lst\"][:, 0];\n",
    "        div_source2 = data[\"lst\"][:, 12:15];\n",
    "        div_data = np.concatenate([np.array([[int(k%4==0), int(k%4==1), int(k%4==2), int(k%4==3)] for k in div_source]), div_source2], axis=1);\n",
    "        lst_data = data[\"lst\"][:, 10:12];\n",
    "        train_labels = lst_data;\n",
    "    return train_data, div_data, train_labels;\n",
    "\n",
    "def read_npz_list():\n",
    "    npz_list = [];\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            npz_list.append(os.path.join(root, file));\n",
    "    # reutnr npz_lsit;\n",
    "    return npz_list;\n",
    "\n",
    "train_glob_defined = False;\n",
    "train_glob_max = 0;\n",
    "train_glob_min = 0;\n",
    "\n",
    "def prefilter_data(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered):\n",
    "    global train_glob_defined, train_glob_max, train_glob_min;\n",
    "    nonempty_labels = np.array([i for i, label in enumerate(train_labels_unfiltered) if (label[0] != 0 or label[1] != 0) and not np.isnan(label[0]) and not np.isnan(label[1]) and label[0] > -10 and label[0] < 10 and label[1] > -10 and label[1] < 10]);\n",
    "\n",
    "    train_data = train_data_unfiltered[nonempty_labels];\n",
    "    div_data = div_data_unfiltered[nonempty_labels];\n",
    "    train_labels = train_labels_unfiltered[nonempty_labels];\n",
    "    if not train_glob_defined:\n",
    "        train_glob_max = train_labels.max(axis=0);\n",
    "        train_glob_min = train_labels.min(axis=0);\n",
    "        train_glob_defined = True;\n",
    "    train_labels_normalized = (train_labels - train_glob_min)/(train_glob_max - train_glob_min) * 0.8 * 2 - 1;\n",
    "    \n",
    "    return train_data, div_data, train_labels_normalized;\n",
    "\n",
    "def preprocess_npzs(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered):\n",
    "    train_data, div_data, train_labels = prefilter_data(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered);\n",
    "    \n",
    "    # Make time intervals from training data\n",
    "    if train_data.shape[0]%time_interval > 0:\n",
    "        train_data = train_data[:-(train_data.shape[0]%time_interval)];\n",
    "        div_data = div_data[:-(div_data.shape[0]%time_interval)];\n",
    "        train_labels = train_labels[:-(train_labels.shape[0]%time_interval)];\n",
    "    train_data2 = np.reshape(train_data, (-1, time_interval, train_data.shape[1], train_data.shape[2], train_data.shape[3]))\n",
    "    div_data2 = np.reshape(div_data, (-1, time_interval, div_data.shape[1]))\n",
    "    train_labels2 = np.reshape(train_labels, (-1, time_interval, train_labels.shape[1]))\n",
    "    return train_data2, div_data2, train_labels2;\n",
    "\n",
    "def get_data_shape():\n",
    "    for file in os.listdir(root):\n",
    "        if file.endswith(\".npz\"):\n",
    "            train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered = read_npz(os.path.join(root, file));\n",
    "            train_data, div_data, train_labels = prefilter_data(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered);\n",
    "            if train_data.shape[0] == 0:\n",
    "                continue;\n",
    "            return train_data.shape, div_data.shape, train_labels.shape;\n",
    "    print(\"cannot find npz!! using default shape\");\n",
    "    return (-1, 7, 32, 2), (-1, 7), (-1, 2);\n",
    "\n",
    "def read_some_npzs_and_preprocess(npz_list):\n",
    "    td_list = [];\n",
    "    dd_list = [];\n",
    "    tl_list = [];\n",
    "    for fp in npz_list:\n",
    "        if fp.endswith(\".npz\"):\n",
    "            _td, _dd, _tl = read_npz(fp);\n",
    "            td_list.append(_td);\n",
    "            dd_list.append(_dd);\n",
    "            tl_list.append(_tl);\n",
    "    train_data_unfiltered = np.concatenate(td_list);\n",
    "    div_data_unfiltered = np.concatenate(dd_list);\n",
    "    train_labels_unfiltered = np.concatenate(tl_list);\n",
    "    \n",
    "    train_data2, div_data2, train_labels2 = preprocess_npzs(train_data_unfiltered, div_data_unfiltered, train_labels_unfiltered);\n",
    "    return train_data2, div_data2, train_labels2;\n",
    "\n",
    "def train_test_split(train_data2, div_data2, train_labels2, test_split_count=233):\n",
    "    new_train_data = train_data2[:-test_split_count];\n",
    "    new_div_data = div_data2[:-test_split_count];\n",
    "    new_train_labels = train_labels2[:-test_split_count];\n",
    "    test_data = train_data2[-test_split_count:];\n",
    "    test_div_data = div_data2[-test_split_count:];\n",
    "    test_labels = train_labels2[-test_split_count:];\n",
    "    return (new_train_data, new_div_data, new_train_labels), (test_data, test_div_data, test_labels);\n",
    "\n",
    "train_file_list = read_npz_list();\n",
    "\n",
    "train_shape, div_shape, label_shape = get_data_shape();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The train_glob_max and train_glob_min parameters need to be saved and subsequently used to scale results in momentum prediction.\n",
    "\n",
    "momentum_predictions_output = momentum_model.predict([test_data2, div_data2]);\n",
    "\n",
    "momentum_predictions = (momentum_predictions_output.reshape(-1, 2) + 1) / 2 / 0.8 * (train_glob_max - train_glob_min) + train_glob_min;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "time_distributed_27_input (Inpu (None, 16, 7, 32, 2) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_27 (TimeDistri (None, 16, 6, 31, 16 144         time_distributed_27_input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_28 (TimeDistri (None, 16, 6, 15, 16 0           time_distributed_27[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_29 (TimeDistri (None, 16, 6, 15, 16 0           time_distributed_28[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_30 (TimeDistri (None, 16, 6, 15, 16 0           time_distributed_29[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_31 (TimeDistri (None, 16, 5, 13, 16 1552        time_distributed_30[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_32 (TimeDistri (None, 16, 5, 6, 16) 0           time_distributed_31[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_33 (TimeDistri (None, 16, 5, 6, 16) 0           time_distributed_32[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_34 (TimeDistri (None, 16, 5, 6, 16) 0           time_distributed_33[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_35 (TimeDistri (None, 16, 480)      0           time_distributed_34[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 16, 64)       139520      time_distributed_35[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 16, 7)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 16, 71)       0           lstm_3[0][0]                     \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 16, 71)       5112        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 16, 71)       5112        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 16, 2)        144         dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 151,584\n",
      "Trainable params: 151,584\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model;\n",
    "\n",
    "def build_model():\n",
    "    model1 = keras.Sequential([\n",
    "        keras.layers.TimeDistributed(keras.layers.Conv2D(16, (2, 2),\n",
    "                           data_format='channels_last'),\n",
    "                           input_shape=(time_interval, train_shape[1], train_shape[2], train_shape[3])),\n",
    "        keras.layers.TimeDistributed(keras.layers.MaxPool2D((1, 2),\n",
    "                           data_format='channels_last')),\n",
    "        keras.layers.TimeDistributed(keras.layers.Activation(activation=tf.nn.relu)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dropout(0.3)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Conv2D(16, (2, 3),\n",
    "                           data_format='channels_last')),\n",
    "        keras.layers.TimeDistributed(keras.layers.MaxPool2D((1, 2),\n",
    "                           data_format='channels_last')),\n",
    "        keras.layers.TimeDistributed(keras.layers.Activation(activation=tf.nn.relu)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Dropout(0.3)),\n",
    "        keras.layers.TimeDistributed(keras.layers.Flatten()),\n",
    "        keras.layers.LSTM(64, activation=tf.nn.tanh, return_sequences=True)\n",
    "    ])\n",
    "    \n",
    "    input2 = keras.layers.InputLayer(input_shape=(time_interval, div_shape[1]));\n",
    "    \n",
    "    conc = keras.layers.concatenate([model1.output, input2.output]);\n",
    "    dense1 = keras.layers.Dense(71, activation=tf.nn.tanh)(conc);\n",
    "    dense2 = keras.layers.Dense(71, activation=tf.nn.relu)(dense1);\n",
    "    dense3 = keras.layers.Dense(label_shape[1], activation=tf.nn.tanh)(dense2);\n",
    "    \n",
    "#     model2 = keras.Sequential([\n",
    "#         keras.layers.Concatenate(64, activation=tf.nn.tanh),\n",
    "#         keras.layers.Dense(64, activation=tf.nn.tanh),\n",
    "#         keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "#         #    keras.layers.Reshape((4,8)),\n",
    "#         #    keras.layers.Conv1D(8,2),\n",
    "#         #    keras.layers.Flatten(),\n",
    "#         keras.layers.Dense(6, activation=tf.nn.tanh)\n",
    "#     ])\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001);\n",
    "\n",
    "    \n",
    "    final_model = Model(inputs=[model1.input, input2.input], outputs=dense3);\n",
    "    final_model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=[keras.metrics.mae])\n",
    "    return final_model\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Limitless]')\n",
    "    plt.plot(history.epoch, np.array(history.history['mean_absolute_error']), \n",
    "           label='Train MAE')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),\n",
    "           label = 'Val MAE')\n",
    "    plt.plot(history.epoch, np.array(history.history['loss']), \n",
    "           label='Train Loss')\n",
    "    plt.plot(history.epoch, np.array(history.history['val_loss']),\n",
    "           label = 'Val Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Display training progress by printing a single dot for each completed epoch.\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        if epoch % 100 == 0: print('')\n",
    "        print('.', end='')\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ぐるぐる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "........\n",
      "Testing set Mean Abs Error: 0.247743009050005\n"
     ]
    }
   ],
   "source": [
    "# Don't worry, it will successfully overfit after those 16 epochs.\n",
    "EPOCHS = 8\n",
    "\n",
    "# since each map npz is about 6mb, this amounts to around 1200mb of RAM.\n",
    "too_many_maps_threshold = 200\n",
    "data_split_count = 160\n",
    "\n",
    "# if there is too much data, reduce epoch count (hmm)\n",
    "if len(train_file_list) >= too_many_maps_threshold:\n",
    "    EPOCHS = 4\n",
    "\n",
    "if len(train_file_list) < too_many_maps_threshold:\n",
    "    train_data2, div_data2, train_labels2 = read_some_npzs_and_preprocess(train_file_list);\n",
    "\n",
    "    # Split some test data out\n",
    "    (new_train_data, new_div_data, new_train_labels), (test_data, test_div_data, test_labels) = train_test_split(train_data2, div_data2, train_labels2);\n",
    "\n",
    "    # Store training stats\n",
    "    history = model.fit([new_train_data, new_div_data], new_train_labels, epochs=EPOCHS,\n",
    "                        validation_split=0.2, verbose=0, #batch_size=477,\n",
    "                        callbacks=[early_stop, PrintDot()])\n",
    "\n",
    "    # For development! may cause bug in some environment.\n",
    "#     plot_history(history)\n",
    "else: # too much data! read it every turn.\n",
    "    for epoch in range(EPOCHS):\n",
    "        for map_batch in range(np.ceil(len(train_file_list) / data_split_count).astype(int)): # hmmmmm\n",
    "            if map_batch == 0:\n",
    "                train_data2, div_data2, train_labels2 = read_some_npzs_and_preprocess(train_file_list[map_batch * data_split_count : (map_batch+1) * data_split_count]);\n",
    "                (new_train_data, new_div_data, new_train_labels), (test_data, test_div_data, test_labels) = train_test_split(train_data2, div_data2, train_labels2);\n",
    "            else:\n",
    "                new_train_data, new_div_data, new_train_labels = read_some_npzs_and_preprocess(train_file_list[map_batch * data_split_count : (map_batch+1) * data_split_count]);\n",
    "            \n",
    "            history = model.fit([new_train_data, new_div_data], new_train_labels, epochs=1,\n",
    "                                validation_split=0.2, verbose=0, #batch_size=477,\n",
    "                                callbacks=[])\n",
    "            # Manually print the dot\n",
    "            print('.', end='');\n",
    "        print('');\n",
    "\n",
    "[loss, mae] = model.evaluate([test_data, test_div_data], test_labels, verbose=0)\n",
    "\n",
    "print(\"\\nTesting set Mean Abs Error: {}\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -0.05883802])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_predictions = model.predict([test_data, test_div_data]).reshape((-1, time_interval, label_shape[1]))\n",
    "\n",
    "# # print(test_predictions)\n",
    "# # print(test_labels)\n",
    "# # print(test_predictions - list(test_labels))\n",
    "# print(\"Mean Abs Error: \"+str(np.mean(np.abs(test_predictions - test_labels))))\n",
    "train_glob_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Save the minmax and the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    }
   ],
   "source": [
    "np.save(\"momentum_minmax\", [train_glob_max, train_glob_min]);\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    \"saved_rhythm_model_momentums\",\n",
    "    overwrite=True,\n",
    "    include_optimizer=True\n",
    ");\n",
    "\n",
    "# WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer\n",
    "# state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will\n",
    "# have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
